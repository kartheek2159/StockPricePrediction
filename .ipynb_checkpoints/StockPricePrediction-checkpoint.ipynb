{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b8f1ac23",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DELL\\anaconda3\\envs\\pytorch\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  Open        High         Low       Close   Adj Close  \\\n",
      "Date                                                                     \n",
      "2001-07-02    2.402500    2.475000    2.400000    2.402500    2.225060   \n",
      "2001-07-03    2.475000    2.475000    2.375000    2.400000    2.222745   \n",
      "2001-07-04    2.425000    2.450000    2.387500    2.420000    2.241267   \n",
      "2001-07-05    2.450000    2.472500    2.400000    2.415000    2.236637   \n",
      "2001-07-06    2.425000    2.425000    2.350000    2.400000    2.222745   \n",
      "...                ...         ...         ...         ...         ...   \n",
      "2016-12-26  711.900024  714.000000  704.250000  711.049988  709.080627   \n",
      "2016-12-27  707.150024  716.000000  705.599976  712.150024  710.177612   \n",
      "2016-12-28  712.000000  718.900024  709.099976  713.049988  711.075134   \n",
      "2016-12-29  710.000000  715.000000  708.349976  713.400024  711.424255   \n",
      "2016-12-30  714.099976  722.750000  714.099976  719.700012  717.706726   \n",
      "\n",
      "               Volume  \n",
      "Date                   \n",
      "2001-07-02   110800.0  \n",
      "2001-07-03    77520.0  \n",
      "2001-07-04    49360.0  \n",
      "2001-07-05    62700.0  \n",
      "2001-07-06   117800.0  \n",
      "...               ...  \n",
      "2016-12-26   485656.0  \n",
      "2016-12-27  1337045.0  \n",
      "2016-12-28  1211967.0  \n",
      "2016-12-29  1303123.0  \n",
      "2016-12-30   990367.0  \n",
      "\n",
      "[3881 rows x 6 columns]\n",
      "                Open      High       Low     Close  Adj Close    Volume\n",
      "Date                                                                   \n",
      "2001-07-02 -1.001200 -1.005773 -0.995874 -1.000711  -0.998021 -0.706888\n",
      "2001-07-03 -1.000878 -1.005773 -0.995986 -1.000722  -0.998031 -0.718293\n",
      "2001-07-04 -1.001100 -1.005882 -0.995930 -1.000634  -0.997949 -0.727944\n",
      "2001-07-05 -1.000989 -1.005784 -0.995874 -1.000656  -0.997970 -0.723372\n",
      "2001-07-06 -1.001100 -1.005992 -0.996098 -1.000722  -0.998031 -0.704489\n",
      "...              ...       ...       ...       ...        ...       ...\n",
      "2016-12-26  2.143982  2.115447  2.152236  2.140797   2.144546 -0.578421\n",
      "2016-12-27  2.122926  2.124220  2.158291  2.145673   2.149423 -0.286640\n",
      "2016-12-28  2.144426  2.136942  2.173990  2.149663   2.153413 -0.329506\n",
      "2016-12-29  2.135560  2.119834  2.170626  2.151215   2.154965 -0.298266\n",
      "2016-12-30  2.153735  2.153830  2.196417  2.179143   2.182896 -0.405450\n",
      "\n",
      "[3881 rows x 6 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DELL\\AppData\\Local\\Temp\\ipykernel_47896\\1967789931.py:14: FutureWarning: DataFrame.mean and DataFrame.median with numeric_only=None will include datetime64 and datetime64tz columns in a future version.\n",
      "  df.fillna(df.mean(), inplace=True)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the CSV file using pandas\n",
    "df = pd.read_csv('./Datasets_input/kotak.csv')\n",
    "\n",
    "# Convert the 'date' column to a datetime object\n",
    "df['Date'] = pd.to_datetime(df['Date'])\n",
    "\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "df.fillna(df.mean(), inplace=True)\n",
    "df.head()\n",
    "\n",
    "# Set the 'date' column as the index\n",
    "df.set_index('Date', inplace=True)\n",
    "# print(df.isnull().sum())\n",
    "\n",
    "\n",
    "\n",
    "# print(df)\n",
    "# Split the data into training, validation, and testing sets\n",
    "train_data = df['2000':'2016']\n",
    "val_data = df['2017':'2018']\n",
    "test_data = df['2019':]\n",
    "print(train_data)\n",
    "# Normalize the data\n",
    "train_mean_original = train_data.mean()\n",
    "train_std_original = train_data.std()\n",
    "train_data = (train_data - train_mean_original) / train_std_original\n",
    "print(train_data)\n",
    "val_data = (val_data - train_mean_original) / train_std_original\n",
    "test_data = (test_data - train_mean_original) / train_std_original\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cbb74735",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StockPriceDataset(Dataset):\n",
    "    def __init__(self, data, seq_len):\n",
    "        self.data = data\n",
    "        self.seq_len = seq_len\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        # Get a sequence of 7 consecutive rows starting from the index\n",
    "        seq = self.data[index:index+self.seq_len]\n",
    "        \n",
    "        # Convert the sequence to a PyTorch tensor\n",
    "        seq_tensor = torch.tensor(seq.values, dtype=torch.float)\n",
    "        \n",
    "        # Return the tensor\n",
    "        return seq_tensor\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data) - self.seq_len + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b60f5933",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_len = 7\n",
    "train_dataset = StockPriceDataset(train_data, seq_len=seq_len)\n",
    "val_dataset = StockPriceDataset(val_data, seq_len=seq_len)\n",
    "test_dataset = StockPriceDataset(test_data, seq_len=seq_len)\n",
    "\n",
    "batch_size=64\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f80b6982",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Original\n",
    "\n",
    "# import torch.nn as nn\n",
    "\n",
    "# class StockPriceModel(nn.Module):\n",
    "#     def __init__(self, input_size, hidden_size, num_layers, kernel_size):\n",
    "#         super(StockPriceModel, self).__init__()\n",
    "        \n",
    "#         # Define the CNN layer\n",
    "#         self.cnn = nn.Conv1d(7, 32, kernel_size=kernel_size, padding=kernel_size//2)\n",
    "        \n",
    "#         # Define the BiLSTM layer\n",
    "#         self.bilstm = nn.LSTM(input_size=32, hidden_size=hidden_size, num_layers=num_layers, batch_first=True, bidirectional=True)\n",
    "        \n",
    "#         # Define the output layer\n",
    "#         self.fc = nn.Linear(hidden_size*2, 1)\n",
    "    \n",
    "#     def forward(self, x):\n",
    "#         # Apply the CNN layer\n",
    "# #         x = x.unsqueeze(1)\n",
    "#         print(\"Input Shape\",x.shape)\n",
    "#         x = self.cnn(x)\n",
    "#         print(\"CNN shape\",x.shape)\n",
    "#         x = nn.functional.relu(x)\n",
    "    \n",
    "#     # Reshape the output of the CNN layer\n",
    "#         batch_size, p, seq_len = x.shape\n",
    "#         x = x.permute(0, 2, 1)\n",
    "#         x = x.reshape(batch_size, seq_len, -1)\n",
    "#         print(\"shape of output of cnn layer is\",x.shape)\n",
    "    \n",
    "#     # Apply the BiLSTM layer\n",
    "#         x, _ = self.bilstm(x)\n",
    "#         print(\"BIlstm output shape\",x.shape)\n",
    "    \n",
    "#     # Apply the output layer\n",
    "#         x = self.fc(x[:, -1, :])\n",
    "#         print(\"Final Output shape\", x.shape)\n",
    "    \n",
    "#         return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7e14732d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# original 2 with changed sequence length\n",
    "\n",
    "# import torch.nn as nn\n",
    "\n",
    "# class StockPriceModel(nn.Module):\n",
    "#     def __init__(self, input_size, hidden_size, num_layers, kernel_size):\n",
    "#         super(StockPriceModel, self).__init__()\n",
    "        \n",
    "#         # Define the CNN layer\n",
    "#         self.cnn = nn.Conv1d(6, 32, kernel_size=kernel_size, padding=kernel_size//2)\n",
    "        \n",
    "#         # Define the BiLSTM layer\n",
    "#         self.bilstm = nn.LSTM(input_size=32, hidden_size=hidden_size, num_layers=num_layers, batch_first=True, bidirectional=True)\n",
    "        \n",
    "#         # Define the output layer\n",
    "#         self.fc = nn.Linear(hidden_size*2, 1)\n",
    "    \n",
    "#     def forward(self, x):\n",
    "#         # Apply the CNN layer\n",
    "# #         x = x.unsqueeze(1)\n",
    "#         print(\"Input Shape\",x.shape)\n",
    "#         x = x.permute(0, 2, 1)\n",
    "#         print(\"shape after permutated\",x.shape)\n",
    "#         x = self.cnn(x)\n",
    "#         print(\"CNN shape\",x.shape)\n",
    "#         x = nn.functional.relu(x)\n",
    "    \n",
    "#     # Reshape the output of the CNN layer\n",
    "#         batch_size, p, seq_len = x.shape\n",
    "#         print(\"Sequence length \",seq_len)\n",
    "#         x = x.permute(0, 2, 1)\n",
    "#         x = x.reshape(batch_size, seq_len, -1)\n",
    "#         print(\"shape of output of cnn layer is\",x.shape)\n",
    "    \n",
    "#     # Apply the BiLSTM layer\n",
    "#         x, _ = self.bilstm(x)\n",
    "#         print(\"BIlstm output shape\",x.shape)\n",
    "    \n",
    "#     # Apply the output layer\n",
    "#         x = self.fc(x[:, -1, :])\n",
    "#         print(\"Final Output shape\", x.shape)\n",
    "    \n",
    "#         return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "78758d97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Original 3 with Added Dense and MaxPool Layer\n",
    "\n",
    "# import torch.nn as nn\n",
    "\n",
    "# class StockPriceModel(nn.Module):\n",
    "#     def __init__(self, input_size, hidden_size, num_layers, kernel_size):\n",
    "#         super(StockPriceModel, self).__init__()\n",
    "        \n",
    "#         # Define the CNN layer\n",
    "#         self.cnn = nn.Conv1d(6, 32, kernel_size=kernel_size, padding=kernel_size//2)\n",
    "        \n",
    "#         # Define the MaxPooling layer\n",
    "#         self.pool = nn.MaxPool1d(kernel_size=2)\n",
    "        \n",
    "#         # Define the BiLSTM layer\n",
    "#         self.bilstm = nn.LSTM(input_size=32, hidden_size=hidden_size, num_layers=num_layers, batch_first=True, bidirectional=True)\n",
    "        \n",
    "#         # Define the Dense layer\n",
    "#         self.dense = nn.Linear(hidden_size*2, 64)\n",
    "        \n",
    "#         # Define the output layer\n",
    "#         self.fc = nn.Linear(64, 1)\n",
    "    \n",
    "#     def forward(self, x):\n",
    "#         # Apply the CNN layer\n",
    "# #         x = x.unsqueeze(1)\n",
    "#         print(\"Input Shape\",x.shape)\n",
    "#         x = x.permute(0, 2, 1)\n",
    "#         print(\"shape after permutated\",x.shape)\n",
    "#         x = self.cnn(x)\n",
    "#         print(\"CNN shape\",x.shape)\n",
    "#         x = nn.functional.relu(x)\n",
    "    \n",
    "#         # Apply the MaxPooling layer\n",
    "#         x = self.pool(x)\n",
    "#         print(\"MaxPooling shape\",x.shape)\n",
    "    \n",
    "#         # Reshape the output of the CNN layer\n",
    "#         batch_size, p, seq_len = x.shape\n",
    "#         print(\"Sequence length \",seq_len)\n",
    "#         x = x.permute(0, 2, 1)\n",
    "#         x = x.reshape(batch_size, seq_len, -1)\n",
    "#         print(\"shape of output of cnn layer is\",x.shape)\n",
    "    \n",
    "#         # Apply the BiLSTM layer\n",
    "#         x, _ = self.bilstm(x)\n",
    "#         print(\"BIlstm output shape\",x.shape)\n",
    "    \n",
    "#         # Apply the Dense layer\n",
    "#         x = self.dense(x[:, -1, :])\n",
    "#         print(\"Dense output shape\",x.shape)\n",
    "        \n",
    "#         # Apply the output layer\n",
    "#         x = self.fc(x)\n",
    "#         print(\"Final Output shape\", x.shape)\n",
    "    \n",
    "#         return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a11488eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class StockPriceModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, kernel_size):\n",
    "        super(StockPriceModel, self).__init__()\n",
    "        \n",
    "        # Define the CNN layer\n",
    "        self.cnn = nn.Conv1d(6, 32, kernel_size=1, padding=0)\n",
    "        \n",
    "        # Define the activation function for CNN layer\n",
    "        self.cnn_activation = nn.Sigmoid()\n",
    "        \n",
    "        # Define the MaxPooling layer\n",
    "        self.pool = nn.MaxPool1d(kernel_size=1, padding=0)\n",
    "        \n",
    "        # Define the BiLSTM layer\n",
    "        self.bilstm = nn.LSTM(input_size=32, hidden_size=hidden_size, num_layers=num_layers, batch_first=True, bidirectional=True)\n",
    "        \n",
    "        # Define the activation function for BiLSTM layer\n",
    "        self.bilstm_activation = nn.Tanh()\n",
    "        \n",
    "        # Define the Dense layer\n",
    "        self.dense = nn.Linear(hidden_size*2, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Apply the CNN layer\n",
    "        x = x.permute(0, 2, 1)\n",
    "        x = self.cnn(x)\n",
    "        x = self.cnn_activation(x)\n",
    "    \n",
    "        # Apply the MaxPooling layer\n",
    "        x = self.pool(x)\n",
    "    \n",
    "        # Reshape the output of the CNN layer\n",
    "        batch_size, p, seq_len = x.shape\n",
    "        x = x.permute(0, 2, 1)\n",
    "        x = x.reshape(batch_size, seq_len, -1)\n",
    "    \n",
    "        # Apply the BiLSTM layer\n",
    "        x, _ = self.bilstm(x)\n",
    "        x = self.bilstm_activation(x)\n",
    "    \n",
    "        # Apply the Dense layer\n",
    "        x = self.dense(x[:, -1, :])\n",
    "        \n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "376b7761",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 7, 6])\n",
      "torch.Size([64, 7, 6])\n",
      "torch.Size([64, 7, 6])\n",
      "torch.Size([64, 7, 6])\n",
      "torch.Size([64, 7, 6])\n",
      "torch.Size([64, 7, 6])\n",
      "torch.Size([64, 7, 6])\n",
      "torch.Size([40, 7, 6])\n",
      "Epoch 1, Train Loss: 0.5329, Val Loss: 0.3545\n",
      "torch.Size([64, 7, 6])\n",
      "torch.Size([64, 7, 6])\n",
      "torch.Size([64, 7, 6])\n",
      "torch.Size([64, 7, 6])\n",
      "torch.Size([64, 7, 6])\n",
      "torch.Size([64, 7, 6])\n",
      "torch.Size([64, 7, 6])\n",
      "torch.Size([40, 7, 6])\n",
      "Epoch 2, Train Loss: 0.5041, Val Loss: 0.3493\n",
      "torch.Size([64, 7, 6])\n",
      "torch.Size([64, 7, 6])\n",
      "torch.Size([64, 7, 6])\n",
      "torch.Size([64, 7, 6])\n",
      "torch.Size([64, 7, 6])\n",
      "torch.Size([64, 7, 6])\n",
      "torch.Size([64, 7, 6])\n",
      "torch.Size([40, 7, 6])\n",
      "Epoch 3, Train Loss: 0.4423, Val Loss: 0.3393\n",
      "torch.Size([64, 7, 6])\n",
      "torch.Size([64, 7, 6])\n",
      "torch.Size([64, 7, 6])\n",
      "torch.Size([64, 7, 6])\n",
      "torch.Size([64, 7, 6])\n",
      "torch.Size([64, 7, 6])\n",
      "torch.Size([64, 7, 6])\n",
      "torch.Size([40, 7, 6])\n",
      "Epoch 4, Train Loss: 0.3151, Val Loss: 0.3532\n",
      "torch.Size([64, 7, 6])\n",
      "torch.Size([64, 7, 6])\n",
      "torch.Size([64, 7, 6])\n",
      "torch.Size([64, 7, 6])\n",
      "torch.Size([64, 7, 6])\n",
      "torch.Size([64, 7, 6])\n",
      "torch.Size([64, 7, 6])\n",
      "torch.Size([40, 7, 6])\n",
      "Epoch 5, Train Loss: 0.2718, Val Loss: 0.3173\n",
      "torch.Size([64, 7, 6])\n",
      "torch.Size([64, 7, 6])\n",
      "torch.Size([64, 7, 6])\n",
      "torch.Size([64, 7, 6])\n",
      "torch.Size([64, 7, 6])\n",
      "torch.Size([64, 7, 6])\n",
      "torch.Size([64, 7, 6])\n",
      "torch.Size([40, 7, 6])\n",
      "Epoch 6, Train Loss: 0.2531, Val Loss: 0.3152\n",
      "torch.Size([64, 7, 6])\n",
      "torch.Size([64, 7, 6])\n",
      "torch.Size([64, 7, 6])\n",
      "torch.Size([64, 7, 6])\n",
      "torch.Size([64, 7, 6])\n",
      "torch.Size([64, 7, 6])\n",
      "torch.Size([64, 7, 6])\n",
      "torch.Size([40, 7, 6])\n",
      "Epoch 7, Train Loss: 0.2415, Val Loss: 0.3033\n",
      "torch.Size([64, 7, 6])\n",
      "torch.Size([64, 7, 6])\n",
      "torch.Size([64, 7, 6])\n",
      "torch.Size([64, 7, 6])\n",
      "torch.Size([64, 7, 6])\n",
      "torch.Size([64, 7, 6])\n",
      "torch.Size([64, 7, 6])\n",
      "torch.Size([40, 7, 6])\n",
      "Epoch 8, Train Loss: 0.2303, Val Loss: 0.3118\n",
      "torch.Size([64, 7, 6])\n",
      "torch.Size([64, 7, 6])\n",
      "torch.Size([64, 7, 6])\n",
      "torch.Size([64, 7, 6])\n",
      "torch.Size([64, 7, 6])\n",
      "torch.Size([64, 7, 6])\n",
      "torch.Size([64, 7, 6])\n",
      "torch.Size([40, 7, 6])\n",
      "Epoch 9, Train Loss: 0.2192, Val Loss: 0.2894\n",
      "torch.Size([64, 7, 6])\n",
      "torch.Size([64, 7, 6])\n",
      "torch.Size([64, 7, 6])\n",
      "torch.Size([64, 7, 6])\n",
      "torch.Size([64, 7, 6])\n",
      "torch.Size([64, 7, 6])\n",
      "torch.Size([64, 7, 6])\n",
      "torch.Size([40, 7, 6])\n",
      "Epoch 10, Train Loss: 0.2113, Val Loss: 0.2948\n",
      "torch.Size([64, 7, 6])\n",
      "torch.Size([64, 7, 6])\n",
      "torch.Size([64, 7, 6])\n",
      "torch.Size([64, 7, 6])\n",
      "torch.Size([64, 7, 6])\n",
      "torch.Size([64, 7, 6])\n",
      "torch.Size([64, 7, 6])\n",
      "torch.Size([40, 7, 6])\n",
      "Epoch 11, Train Loss: 0.1960, Val Loss: 0.2824\n",
      "torch.Size([64, 7, 6])\n",
      "torch.Size([64, 7, 6])\n",
      "torch.Size([64, 7, 6])\n",
      "torch.Size([64, 7, 6])\n",
      "torch.Size([64, 7, 6])\n",
      "torch.Size([64, 7, 6])\n",
      "torch.Size([64, 7, 6])\n",
      "torch.Size([40, 7, 6])\n",
      "Epoch 12, Train Loss: 0.1830, Val Loss: 0.2988\n",
      "torch.Size([64, 7, 6])\n",
      "torch.Size([64, 7, 6])\n",
      "torch.Size([64, 7, 6])\n",
      "torch.Size([64, 7, 6])\n",
      "torch.Size([64, 7, 6])\n",
      "torch.Size([64, 7, 6])\n",
      "torch.Size([64, 7, 6])\n",
      "torch.Size([40, 7, 6])\n",
      "Epoch 13, Train Loss: 0.1692, Val Loss: 0.2616\n",
      "torch.Size([64, 7, 6])\n",
      "torch.Size([64, 7, 6])\n",
      "torch.Size([64, 7, 6])\n",
      "torch.Size([64, 7, 6])\n",
      "torch.Size([64, 7, 6])\n",
      "torch.Size([64, 7, 6])\n",
      "torch.Size([64, 7, 6])\n",
      "torch.Size([40, 7, 6])\n",
      "Epoch 14, Train Loss: 0.1591, Val Loss: 0.2653\n",
      "torch.Size([64, 7, 6])\n",
      "torch.Size([64, 7, 6])\n",
      "torch.Size([64, 7, 6])\n",
      "torch.Size([64, 7, 6])\n",
      "torch.Size([64, 7, 6])\n",
      "torch.Size([64, 7, 6])\n",
      "torch.Size([64, 7, 6])\n",
      "torch.Size([40, 7, 6])\n",
      "Epoch 15, Train Loss: 0.1443, Val Loss: 0.2570\n",
      "torch.Size([64, 7, 6])\n",
      "torch.Size([64, 7, 6])\n",
      "torch.Size([64, 7, 6])\n",
      "torch.Size([64, 7, 6])\n",
      "torch.Size([64, 7, 6])\n",
      "torch.Size([64, 7, 6])\n",
      "torch.Size([64, 7, 6])\n",
      "torch.Size([40, 7, 6])\n",
      "Epoch 16, Train Loss: 0.1326, Val Loss: 0.2332\n",
      "torch.Size([64, 7, 6])\n",
      "torch.Size([64, 7, 6])\n",
      "torch.Size([64, 7, 6])\n",
      "torch.Size([64, 7, 6])\n",
      "torch.Size([64, 7, 6])\n",
      "torch.Size([64, 7, 6])\n",
      "torch.Size([64, 7, 6])\n",
      "torch.Size([40, 7, 6])\n",
      "Epoch 17, Train Loss: 0.1178, Val Loss: 0.2250\n",
      "torch.Size([64, 7, 6])\n",
      "torch.Size([64, 7, 6])\n",
      "torch.Size([64, 7, 6])\n",
      "torch.Size([64, 7, 6])\n",
      "torch.Size([64, 7, 6])\n",
      "torch.Size([64, 7, 6])\n",
      "torch.Size([64, 7, 6])\n",
      "torch.Size([40, 7, 6])\n",
      "Epoch 18, Train Loss: 0.1037, Val Loss: 0.2032\n",
      "torch.Size([64, 7, 6])\n",
      "torch.Size([64, 7, 6])\n",
      "torch.Size([64, 7, 6])\n",
      "torch.Size([64, 7, 6])\n",
      "torch.Size([64, 7, 6])\n",
      "torch.Size([64, 7, 6])\n",
      "torch.Size([64, 7, 6])\n",
      "torch.Size([40, 7, 6])\n",
      "Epoch 19, Train Loss: 0.0912, Val Loss: 0.1993\n",
      "torch.Size([64, 7, 6])\n",
      "torch.Size([64, 7, 6])\n",
      "torch.Size([64, 7, 6])\n",
      "torch.Size([64, 7, 6])\n",
      "torch.Size([64, 7, 6])\n",
      "torch.Size([64, 7, 6])\n",
      "torch.Size([64, 7, 6])\n",
      "torch.Size([40, 7, 6])\n",
      "Epoch 20, Train Loss: 0.0840, Val Loss: 0.1771\n"
     ]
    }
   ],
   "source": [
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = StockPriceModel(input_size=6, hidden_size=128, num_layers=2, kernel_size=5).to(device)\n",
    "criterion = nn.L1Loss() #mean absolute error\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
    "train_losses=[]\n",
    "val_losses=[]\n",
    "\n",
    "\n",
    "num_epochs = 50\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = 0.0\n",
    "    for batch in train_loader:\n",
    "        # Move the batch to the GPU\n",
    "        batch = batch.to(device)\n",
    "#         print(batch.shape) \n",
    "        outputs = model(batch)\n",
    "        targets = batch[:, -1, -1].unsqueeze(1)\n",
    "        loss = criterion(outputs, targets)\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item() * batch.shape[0]\n",
    "    \n",
    "    # Evaluate the model on the validation set\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            # Move the batch to the GPU\n",
    "            batch = batch.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            print(batch.shape)\n",
    "            outputs = model(batch)\n",
    "            targets = batch[:, -1, -1].unsqueeze(1)\n",
    "            loss = criterion(outputs, targets)\n",
    "            val_loss += loss.item() * batch.shape[0]\n",
    "    \n",
    "    train_loss /= len(train_data)\n",
    "    val_loss /= len(val_data)\n",
    "    train_losses.append(train_loss)\n",
    "    val_losses.append(val_loss)\n",
    "    print(f'Epoch {epoch+1}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7125d53c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(1,num_epochs+1),train_losses,label='Training Loss')\n",
    "plt.plot(range(1,num_epochs+1),val_losses,label='Validation Loss')\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6cd0661",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on the test data\n",
    "test_loss = 0\n",
    "predictions = []\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    for batch in test_loader:\n",
    "        # Move the batch to the device\n",
    "        batch = batch.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        output = model(batch)\n",
    "        \n",
    "        # Calculate the loss\n",
    "        loss = criterion(output.squeeze(), batch[:, -1, 3])\n",
    "        test_loss += loss.item() * batch.shape[0]\n",
    "        \n",
    "        # Add the predictions to the list\n",
    "        predictions += output.squeeze().detach().cpu().numpy().tolist()\n",
    "#         print(predictions)\n",
    "\n",
    "test_loss /= len(test_data)\n",
    "print('Test Loss:', test_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "055f8f8a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5ee841c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "print(torch.cuda.is_available())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f3d2565",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
